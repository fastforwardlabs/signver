{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SignVer_Test_Harness.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQFO4qAxOlg9"
      },
      "source": [
        "# SignVer Test Harness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwQvSrZdOjgA"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import itertools\n",
        "from typing import Tuple\n",
        "\n",
        "import PIL\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# %load_ext nb_black"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yieyp-eZQZPS"
      },
      "source": [
        "## Load data and generate pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRDMvybLOrzT"
      },
      "source": [
        "class SignatureDataset:\n",
        "    \"\"\"\n",
        "    Captures a signature dataset into an abstract, representative collection.\n",
        "\n",
        "    Currently supported datasets are: CEDAR Signatures\n",
        "\n",
        "    Attributes:\n",
        "        root_dir: path to the root directory location of the dataset\n",
        "        dataset: naming identifier for the dataset ('cedar')\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, dataset_name):\n",
        "        self.root_dir = root_dir\n",
        "        self.dataset = dataset_name\n",
        "\n",
        "        self.images, self.image_metadata = self.collect_cedar_images(\n",
        "            # root_dir, target_shape=(128, 128)\n",
        "            root_dir, target_shape=(224, 224)\n",
        "        )\n",
        "        self.img_path_lookup = self.image_metadata.image_path.to_dict()\n",
        "        self.assign_train_test_split()\n",
        "        self.verification_pairs = self.generate_verification_pairs(self.image_metadata)\n",
        "\n",
        "    @classmethod\n",
        "    def collect_cedar_images(self, root_path: str, target_shape: Tuple):\n",
        "        \"\"\"\n",
        "        Collates CEDAR Signature Dataset into a representative dictionary.\n",
        "\n",
        "        Args:\n",
        "            root_path: the root path of a locally saved CEDAR dataset (https://cedar.buffalo.edu/NIJ/data/)\n",
        "            target_shape: desired image shape for loading imaeges\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        images = []\n",
        "        image_metadata = []\n",
        "\n",
        "        for author in range(1, 56):\n",
        "            for subfolder in [\"full_org\", \"full_forg\"]:\n",
        "                for sig_number in range(1, 25):\n",
        "\n",
        "                    prefix = \"original\" if subfolder == \"full_org\" else \"forgeries\"\n",
        "                    filename = f\"{prefix}_{author}_{sig_number}.png\"\n",
        "                    img_path = os.path.join(root_path, subfolder, filename)\n",
        "\n",
        "                    # img = load_img(\n",
        "                    #     path=img_path, target_size=target_shape, color_mode=\"grayscale\"\n",
        "                    # )\n",
        "                    # arr = img_to_array(img).reshape(1, *target_shape)\n",
        "\n",
        "                    # images.append(arr)\n",
        "                    image_metadata.append(\n",
        "                        (\n",
        "                            author,\n",
        "                            \"original\" if subfolder == \"full_org\" else \"forgery\",\n",
        "                            sig_number,\n",
        "                            img_path,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "        # images = np.vstack(images)\n",
        "        image_metadata = pd.DataFrame(\n",
        "            image_metadata, columns=[\"author\", \"label\", \"sig_number\", \"image_path\"]\n",
        "        )\n",
        "\n",
        "        return images, image_metadata\n",
        "\n",
        "    def assign_train_test_split(self, train_size=0.8):\n",
        "        \"\"\"\n",
        "        Assign train vs test labels by author to self.image_metadata\n",
        "\n",
        "        \"\"\"\n",
        "        random.seed(42)\n",
        "\n",
        "        authors = self.image_metadata.author.unique()\n",
        "        random.shuffle(authors)\n",
        "\n",
        "        train_size = int(np.floor(train_size * len(authors)))\n",
        "        train, test = authors[:train_size], authors[train_size:]\n",
        "\n",
        "        self.image_metadata[\"train_test\"] = self.image_metadata.author.apply(\n",
        "            lambda x: \"train\" if x in train else \"test\"\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def generate_verification_pairs(self, image_metadata):\n",
        "        \"\"\"\n",
        "        Generates a list of verification pairs (genuine/genuine, genuine/forged, genuine/unskilled_forged) for each author\n",
        "        from a given metadata dataframe.\n",
        "\n",
        "        To maintain a balanced dataset, genuine/forged and genuine/unskilled pairs are randomly sampled down\n",
        "        to maintain the same number of examples as genuine/genuine.\n",
        "\n",
        "        Args:\n",
        "            image_metadata: dataframe of labels where index matches each feature index in features\n",
        "\n",
        "        Returns:\n",
        "            author_dfs: dataframe of signature pairs (as specified by index into metadata dataframe) for each author and label type\n",
        "\n",
        "        TO-DO - return this as a shuffled, useable set of data for training/inference\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        np.random.seed(42)\n",
        "        author_dfs = []\n",
        "\n",
        "        for author in image_metadata.author.unique():\n",
        "\n",
        "            author_subset = image_metadata[image_metadata.author == author]\n",
        "\n",
        "            genuine_idx_array = author_subset[\n",
        "                author_subset.label == \"original\"\n",
        "            ].index.values\n",
        "\n",
        "            forged_idx_array = author_subset[\n",
        "                author_subset.label == \"forgery\"\n",
        "            ].index.values\n",
        "\n",
        "            unskilled_index_array = image_metadata[\n",
        "                ~image_metadata.index.isin(\n",
        "                    np.concatenate([genuine_idx_array, forged_idx_array])\n",
        "                )\n",
        "            ].index.values\n",
        "\n",
        "            # identify (genuine, genuine) combinatorial pairs\n",
        "            genuine_idx_combinations = list(\n",
        "                itertools.combinations(genuine_idx_array, 2)\n",
        "            )\n",
        "\n",
        "            # identify (genuine, forged) cartesian product pairs; sample down to have same number pairs as genuine\n",
        "            forged_idx_combinations = np.array(\n",
        "                list(itertools.product(genuine_idx_array, forged_idx_array))\n",
        "            )\n",
        "            forged_indicies = np.random.choice(\n",
        "                list(range(len(forged_idx_combinations))),\n",
        "                size=len(genuine_idx_combinations),\n",
        "                replace=False,\n",
        "            )\n",
        "            forged_idx_combinations = forged_idx_combinations[forged_indicies].tolist()\n",
        "\n",
        "            # identify (genuine, unskilled_forged) pairs by randomly sampling from list of OTHER authors signatures\n",
        "            unskilled_idx_combinations = list(\n",
        "                zip(\n",
        "                    np.random.choice(\n",
        "                        genuine_idx_array,\n",
        "                        size=len(genuine_idx_combinations),\n",
        "                        replace=True,\n",
        "                    ),\n",
        "                    np.random.choice(\n",
        "                        unskilled_index_array,\n",
        "                        size=len(genuine_idx_combinations),\n",
        "                        replace=False,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "            sig_pairs = {\n",
        "                0: forged_idx_combinations,  # for contrastive loss, dissimilar pairs are label 0\n",
        "                1: genuine_idx_combinations,\n",
        "                2: unskilled_idx_combinations,\n",
        "            }\n",
        "\n",
        "            for label, combs in sig_pairs.items():\n",
        "                author_df = pd.DataFrame(\n",
        "                    sig_pairs[label],\n",
        "                    columns=[\"anchor_idx\", \"alt_idx\"],\n",
        "                )\n",
        "                author_df[\"author\"] = author\n",
        "                author_df[\"label\"] = label\n",
        "\n",
        "                author_dfs.append(author_df)\n",
        "\n",
        "        return pd.concat(author_dfs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMbUi50COwDc"
      },
      "source": [
        "%%time\n",
        "\n",
        "# CEDAR_PATH = \"/content/gdrive/MyDrive/FF20: SignVer/data/cedar\"\n",
        "CEDAR_PATH = \"/content/local_data/cedar\"\n",
        "\n",
        "cedar_dataset = SignatureDataset(root_dir=CEDAR_PATH, dataset_name=\"cedar\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl7fUNSWQVrn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faujtU5zQW9i"
      },
      "source": [
        "## Identify Validation Pairs Only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07XRWsufQQSw"
      },
      "source": [
        "def gather_validation_pairs(verification_pairs, image_metadata, idx_filename_mapping):\n",
        "    \"\"\"\n",
        "    Given a dataframe of verification pairs for each author and mapping of index to filepaths,\n",
        "    this function collects image pairs (filenames) and labels (int) and returns them as a\n",
        "    nested tf.data.Dataset object.\n",
        "\n",
        "    \"\"\"\n",
        "    authors = image_metadata[image_metadata.train_test == 'test'].author.unique()\n",
        "    verification_pairs = verification_pairs[(verification_pairs.author.isin(authors)) & (verification_pairs.label != 2)]\n",
        "\n",
        "    verification_pairs.anchor_idx = verification_pairs.anchor_idx.apply(lambda x: idx_filename_mapping[x])\n",
        "    verification_pairs.alt_idx = verification_pairs.alt_idx.apply(lambda x: idx_filename_mapping[x])\n",
        "\n",
        "    return verification_pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdT7eRSyQSgD"
      },
      "source": [
        "gather_validation_pairs(cedar_dataset.verification_pairs,\n",
        "                        cedar_dataset.image_metadata,\n",
        "                        cedar_dataset.img_path_lookup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asBG05b1QlYZ"
      },
      "source": [
        "## Calculate similarity\n",
        "\n",
        "NOTE - I don't have a function that does this for you, but basically for the image pairs in the dataframe above, you'll need to:\n",
        "1. load the image from its filepath\n",
        "2. preprocess each image\n",
        "3. get embeddings from your model for each image\n",
        "4. calculate cosine distance on each pair\n",
        "\n",
        "\n",
        "This will leave you with y_pred, and the y_true is the \"label\" column in the dataframe..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x29R7-WkQ_0r"
      },
      "source": [
        "## Characterize Performance\n",
        "\n",
        "Then using the y_pred and y_true, run the following functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnDsakISQ_AC"
      },
      "source": [
        "def find_eer(fprs, tprs, thresholds):\n",
        "    \"\"\"\n",
        "    Given a set of False Positive Rates, True Positive Rates, and corresponding thresholds output by\n",
        "    sklearn.metrics.roc_curve(), calculate the Equal Error Rate (EER).\n",
        "\n",
        "    EER (also known as Crossover Error Rate) is the point on the ROC curve where FPR==FNR.\n",
        "\n",
        "    \"\"\"\n",
        "    fnrs = 1 - tprs\n",
        "\n",
        "    crossover = np.argwhere(np.diff(np.sign(fprs - fnrs)))\n",
        "\n",
        "    if len(crossover) > 1:\n",
        "        crossover_idx = np.argwhere(np.diff(np.sign(fprs - fnrs)))[0].item()\n",
        "    else:\n",
        "        crossover_idx = np.argwhere(np.diff(np.sign(fprs - fnrs))).item()\n",
        "\n",
        "    crossover_thresh = thresholds[crossover_idx]\n",
        "    crossover_error = fnrs[crossover_idx]\n",
        "\n",
        "    return crossover_idx, crossover_thresh, crossover_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL1Yor1pRM1t"
      },
      "source": [
        "def find_max_accuracy(fprs, tprs, thresholds):\n",
        "\n",
        "    accs = []\n",
        "\n",
        "    for fpr, tpr, thresh in list(zip(fprs, tprs, thresholds)):\n",
        "\n",
        "        accuracy = 0.5 * (tpr + (1 - fpr))  # tnr = (1-fpr)\n",
        "        accs.append(accuracy)\n",
        "\n",
        "    max_acc_idx = np.argmax(accs)\n",
        "\n",
        "    return accs[max_acc_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlDfYmHjROEz"
      },
      "source": [
        "def get_metrics(fprs, tprs, thresholds):\n",
        "\n",
        "  crossover_idx, crossover_thresh, crossover_error = find_eer(fprs, tprs, thresholds)\n",
        "  max_accuracy = find_max_accuracy(fprs, tprs, thresholds)\n",
        "\n",
        "  return {'crossover_idx': crossover_idx,\n",
        "          'crossover_thresh': crossover_thresh,\n",
        "          'crossover_error': crossover_error,\n",
        "          'max_accuracy': max_accuracy}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3DuOUleRP_s"
      },
      "source": [
        "fprs, tprs, thresholds = roc_curve(\n",
        "    y_true=records_triplet.label, y_score=records_triplet.distance, pos_label=0\n",
        ")\n",
        "\n",
        "fnrs = 1 - tprs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zWP2HOmRN-e"
      },
      "source": [
        "plt.plot(fprs, tprs)\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.xlabel(\"False Positive Rate (FPR)\")\n",
        "plt.ylabel(\"True Positive Rate (TPR)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awRNFTtXRWcE"
      },
      "source": [
        "get_metrics(fprs, tprs, thresholds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXlqrGKJRWWG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}